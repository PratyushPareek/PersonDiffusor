{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8825413,"sourceType":"datasetVersion","datasetId":5309711}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n%pip install -q -U --pre triton\n%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers\n!pip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T18:33:04.911795Z","iopub.execute_input":"2024-06-30T18:33:04.912286Z","iopub.status.idle":"2024-06-30T18:35:15.644537Z","shell.execute_reply.started":"2024-06-30T18:33:04.912250Z","shell.execute_reply":"2024-06-30T18:35:15.643373Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.1.0 requires triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 2.3.1 which is incompatible.\nxformers 0.0.26.post1 requires torch==2.3.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nRequirement already satisfied: jax==0.4.23 in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]==0.4.23) (0.4.23)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (0.2.0)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (1.26.4)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (1.11.4)\n\u001b[33mWARNING: jax 0.4.23 does not provide the extra 'cuda12-pip'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: jaxlib==0.4.23+cuda12.cudnn89 in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]==0.4.23) (0.4.23+cuda12.cudnn89)\nCollecting nvidia-cublas-cu12>=12.2.5.6 (from jax[cuda12_pip]==0.4.23)\n  Using cached nvidia_cublas_cu12-12.5.2.13-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12>=12.2.142 (from jax[cuda12_pip]==0.4.23)\n  Using cached nvidia_cuda_cupti_cu12-12.5.39-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cuda-nvcc-cu12>=12.2.140 in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]==0.4.23) (12.5.40)\nCollecting nvidia-cuda-runtime-cu12>=12.2.140 (from jax[cuda12_pip]==0.4.23)\n  Using cached nvidia_cuda_runtime_cu12-12.5.39-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: nvidia-cudnn-cu12>=8.9 in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]==0.4.23) (8.9.2.26)\nCollecting nvidia-cufft-cu12>=11.0.8.103 (from jax[cuda12_pip]==0.4.23)\n  Using cached nvidia_cufft_cu12-11.2.3.18-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12>=11.5.2 (from jax[cuda12_pip]==0.4.23)\n  Using cached nvidia_cusolver_cu12-11.6.2.40-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12>=12.1.2.141 (from jax[cuda12_pip]==0.4.23)\n  Using cached nvidia_cusparse_cu12-12.4.1.24-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12>=2.18.3 in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]==0.4.23) (2.20.5)\nRequirement already satisfied: nvidia-nvjitlink-cu12>=12.2 in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]==0.4.23) (12.5.40)\nUsing cached nvidia_cublas_cu12-12.5.2.13-py3-none-manylinux2014_x86_64.whl (363.3 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.5.39-py3-none-manylinux2014_x86_64.whl (13.8 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.5.39-py3-none-manylinux2014_x86_64.whl (895 kB)\nUsing cached nvidia_cufft_cu12-11.2.3.18-py3-none-manylinux2014_x86_64.whl (192.5 MB)\nUsing cached nvidia_cusolver_cu12-11.6.2.40-py3-none-manylinux2014_x86_64.whl (130.3 MB)\nUsing cached nvidia_cusparse_cu12-12.4.1.24-py3-none-manylinux2014_x86_64.whl (209.2 MB)\nInstalling collected packages: nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.3.0 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.2.13 which is incompatible.\ntorch 2.3.0 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.39 which is incompatible.\ntorch 2.3.0 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.39 which is incompatible.\ntorch 2.3.0 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.18 which is incompatible.\ntorch 2.3.0 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.2.40 which is incompatible.\ntorch 2.3.0 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.4.1.24 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.5.2.13 nvidia-cuda-cupti-cu12-12.5.39 nvidia-cuda-runtime-cu12-12.5.39 nvidia-cufft-cu12-11.2.3.18 nvidia-cusolver-cu12-11.6.2.40 nvidia-cusparse-cu12-12.4.1.24\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch==2.1.0\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:25:19.343446Z","iopub.execute_input":"2024-06-30T18:25:19.344233Z","iopub.status.idle":"2024-06-30T18:26:36.026879Z","shell.execute_reply.started":"2024-06-30T18:25:19.344194Z","shell.execute_reply":"2024-06-30T18:26:36.025682Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting torch==2.1.0\n  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (2024.3.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (12.1.105)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (8.9.2.26)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (10.3.2.106)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (12.1.105)\nCollecting triton==2.1.0 (from torch==2.1.0)\n  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.5.40)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0) (1.3.0)\nDownloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\nUsing cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\nUsing cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\nUsing cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\nDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 2.3.0\n    Uninstalling triton-2.3.0:\n      Successfully uninstalled triton-2.3.0\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.20.5\n    Uninstalling nvidia-nccl-cu12-2.20.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.4.1.24\n    Uninstalling nvidia-cusparse-cu12-12.4.1.24:\n      Successfully uninstalled nvidia-cusparse-cu12-12.4.1.24\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.18\n    Uninstalling nvidia-cufft-cu12-11.2.3.18:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.18\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.39\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.39:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.39\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.39\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.39:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.39\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.2.13\n    Uninstalling nvidia-cublas-cu12-12.5.2.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.2.13\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.2.40\n    Uninstalling nvidia-cusolver-cu12-11.6.2.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.2.40\n  Attempting uninstall: torch\n    Found existing installation: torch 2.3.0\n    Uninstalling torch-2.3.0:\n      Successfully uninstalled torch-2.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nxformers 0.0.26.post1 requires torch==2.3.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 torch-2.1.0 triton-2.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:19:33.934034Z","iopub.execute_input":"2024-06-30T18:19:33.934952Z","iopub.status.idle":"2024-06-30T18:19:34.223318Z","shell.execute_reply.started":"2024-06-30T18:19:33.934891Z","shell.execute_reply":"2024-06-30T18:19:34.222468Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ec0dc2bb924837807b76751e520b88"}},"metadata":{}}]},{"cell_type":"code","source":"import os\nos.environ['MODEL_NAME'] = \"runwayml/stable-diffusion-v1-5\"\nos.environ['DATA_DIR'] = \"/kaggle/input/aaryan\"\nos.environ['OUTPUT_DIR'] = \"/kaggle/working/model\"","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:35:33.691273Z","iopub.execute_input":"2024-06-30T18:35:33.691642Z","iopub.status.idle":"2024-06-30T18:35:33.696884Z","shell.execute_reply.started":"2024-06-30T18:35:33.691611Z","shell.execute_reply":"2024-06-30T18:35:33.695834Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!python3 train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n  --instance_data_dir=$DATA_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --revision=\"fp16\" \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --seed=1337 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --train_text_encoder \\\n  --mixed_precision=\"fp16\" \\\n  --use_8bit_adam \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=50 \\\n  --sample_batch_size=4 \\\n  --max_train_steps=800 \\\n  --save_interval=10000 \\\n  --save_sample_prompt=\"photo of aaryan person\"","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:35:35.634646Z","iopub.execute_input":"2024-06-30T18:35:35.635502Z","iopub.status.idle":"2024-06-30T18:35:36.760891Z","shell.execute_reply.started":"2024-06-30T18:35:35.635472Z","shell.execute_reply":"2024-06-30T18:35:36.759778Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/kaggle/working/train_dreambooth.py\", line 13, in <module>\n    import torch\n  File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 237, in <module>\n    from torch._C import *  # noqa: F403\nImportError: /opt/conda/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_5, version libnvJitLink.so.12\n","output_type":"stream"}]},{"cell_type":"code","source":"# inference\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\npipe = StableDiffusionPipeline.from_pretrained(os.environ.get(\"PROJECT_NAME\"), torch_dtype=torch.float16).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:19:35.390242Z","iopub.execute_input":"2024-06-30T18:19:35.390618Z","iopub.status.idle":"2024-06-30T18:19:36.510573Z","shell.execute_reply.started":"2024-06-30T18:19:35.390589Z","shell.execute_reply":"2024-06-30T18:19:36.509283Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROJECT_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m), torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.15.0.dev0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigMixin\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m      6\u001b[0m     is_flax_available,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     logging,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/configuration_utils.py:34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     DIFFUSERS_CACHE,\n\u001b[1;32m     36\u001b[0m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[1;32m     37\u001b[0m     DummyObject,\n\u001b[1;32m     38\u001b[0m     deprecate,\n\u001b[1;32m     39\u001b[0m     extract_commit_hash,\n\u001b[1;32m     40\u001b[0m     http_user_agent,\n\u001b[1;32m     41\u001b[0m     logging,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     45\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     47\u001b[0m _re_configuration_file \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.(.*)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/utils/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerate_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_forward_hook\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     CONFIG_NAME,\n\u001b[1;32m     24\u001b[0m     DEPRECATED_REVISION_ARGS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:20\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mAccelerate utilities: Utilities related to accelerate\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_accelerate_available\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/utils/import_utils.py:209\u001b[0m\n\u001b[1;32m    207\u001b[0m _xformers_version \u001b[38;5;241m=\u001b[39m importlib_metadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxformers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_available:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mVersion(torch\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mVersion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.12\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch should be >= 1.12\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:237\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    236\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n","\u001b[0;31mImportError\u001b[0m: /opt/conda/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_5, version libnvJitLink.so.12"],"ename":"ImportError","evalue":"/opt/conda/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_5, version libnvJitLink.so.12","output_type":"error"}]},{"cell_type":"code","source":"counter= 0","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:19:36.511859Z","iopub.status.idle":"2024-06-30T18:19:36.512243Z","shell.execute_reply.started":"2024-06-30T18:19:36.512069Z","shell.execute_reply":"2024-06-30T18:19:36.512083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter+=1\n\nprompt = \"a photo of aaryan\"\noutput_image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\noutput_image.save(f\"new_portrait_{counter}.png\")\noutput_image","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:19:36.513175Z","iopub.status.idle":"2024-06-30T18:19:36.513493Z","shell.execute_reply.started":"2024-06-30T18:19:36.513331Z","shell.execute_reply":"2024-06-30T18:19:36.513344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"a photo of aaryan in van gogh style\"\noutput_image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n\noutput_image.save(\"new_portrait_van_gogh.png\")\noutput_image","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:19:36.514615Z","iopub.status.idle":"2024-06-30T18:19:36.514965Z","shell.execute_reply.started":"2024-06-30T18:19:36.514777Z","shell.execute_reply":"2024-06-30T18:19:36.514791Z"},"trusted":true},"execution_count":null,"outputs":[]}]}